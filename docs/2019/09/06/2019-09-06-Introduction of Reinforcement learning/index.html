<!DOCTYPE html>
<html lang="en">
<head><meta name="generator" content="Hexo 3.9.0">
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="google-site-verification" content="xBT4GhYoi5qRD5tr338pgPM5OWHHIDR6mNg1a3euekI">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content>
    <meta name="keyword" content>
    <link rel="shortcut icon" href="/blog/img/favicon.ico">

    <title>
        
        Introduction of Reinforcement learning - undefined
        
    </title>

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/blog/css/aircloud.css">
    <link rel="stylesheet" href="/blog/css/gitment.css">
    <!--<link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css">-->
    <link href="//at.alicdn.com/t/font_620856_pl6z7sid89qkt9.css" rel="stylesheet" type="text/css">
    <!-- ga & ba script hoook -->
    <script></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>

<body>

<!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="site-nav-toggle" id="site-nav-toggle">
    <button>
        <span class="btn-bar"></span>
        <span class="btn-bar"></span>
        <span class="btn-bar"></span>
    </button>
</div>

<div class="index-about">
    <i> 浙江大学可视分析小组博客 </i>
</div>

<div class="index-container">
    
    <div class="index-left">
        <div class="nav" id="nav">
    <div class="avatar-name">
        <div class="avatar ">
            <a href="http://zjuvag.org"><img
                    src="https://zjuvag.org/authors/admin/avatar_hue342753856f1b55bb62d38c7380e0dc0_121560_250x250_fill_q90_lanczos_center.jpg" /></a>
        </div>
        <div class="name">
            <a href="http://zjuvag.org">
                <i>ZJU VAG</i>
            </a>
        </div>
    </div>
    <div class="contents" id="nav-content">
        <ul>
            <li >
                <a
                    href="http://zjuvag.org">
                    <!-- <a href="/blog/about/"> -->
                    <i class="iconfont icon-fanhui"></i>
                    <span>小组主页</span>
                </a>
            </li>
            <li >
                <a href="/blog/">
                    <i class="iconfont icon-shouye1"></i>
                    <span>小组博客</span>
                </a>
            </li>
            <li >
                <a href="/blog/tags">
                    <i class="iconfont icon-biaoqian1"></i>
                    <span>博客标签</span>
                </a>
            </li>
            <li >
                <a href="/blog/author">
                    <i class="iconfont icon-guanyu2"></i>
                    <span>作者存档</span>
                </a>
            </li>
            <li >
                <a href="/blog/archives">
                    <i class="iconfont icon-guidang2"></i>
                    <span>时间存档</span>
                </a>
            </li>

            <li >
                <a href="/blog/textbook">
                    <i class="iconfont icon-biaoqian1"></i>
                    <span>教材下载</span>
                </a>
            </li>
            
            <li>
                <a id="search">
                    <i class="iconfont icon-sousuo1"></i>
                    <span>博客搜索</span>
                </a>
            </li>
            
        </ul>
    </div>
    
    <div id="toc" class="toc-article">
    <ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#Markov-Process"><span class="toc-text">Markov Process</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Definition"><span class="toc-text">Definition</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Markov-Reward-Process"><span class="toc-text">Markov Reward Process</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#G-t"><span class="toc-text">$G_t$</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#价值函数-Value-Function"><span class="toc-text">价值函数(Value Function)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#贝尔曼方程-Bellman-equation"><span class="toc-text">贝尔曼方程(Bellman equation)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#矩阵求解"><span class="toc-text">矩阵求解</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Markov-Decision-Process"><span class="toc-text">Markov Decision Process</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#策略-policy"><span class="toc-text">策略(policy)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#价值函数-2"><span class="toc-text">价值函数-2</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#将其最优化"><span class="toc-text">将其最优化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#最优策略"><span class="toc-text">最优策略</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#最优状态动作价值函数"><span class="toc-text">最优状态动作价值函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#如何求解"><span class="toc-text">如何求解</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#On-policy-vs-Off-policy"><span class="toc-text">On-policy vs Off-policy</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Policy"><span class="toc-text">Policy</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Policy-Gradient"><span class="toc-text">Policy Gradient</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#gt-policy-gradient-theorem"><span class="toc-text">==&gt;policy gradient theorem</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#gt-李宏毅教授PPT"><span class="toc-text">==&gt;李宏毅教授PPT</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Actor-Critic"><span class="toc-text">Actor-Critic</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Reference"><span class="toc-text">Reference</span></a></li></ol>
</div>
    
</div>


<div class="search-field" id="search-field">
    <div class="search-container">
        <div class="search-input">
            <span id="esc-search"> <i class="icon-fanhui iconfont"></i></span>
            <input id="search-input" />
            <span id="begin-search">搜索</span>
        </div>
        <div class="search-result-container" id="search-result-container">

        </div>
    </div>
</div>
        <div class="index-about-mobile">
            <i> 浙江大学可视分析小组博客 </i>
        </div>
    </div>
    
    <div class="index-middle">
        <!-- Main Content -->
        <div class="post-container">
    <div class="post-title">
        Introduction of Reinforcement learning
    </div>

    <div class="post-meta">
        <!-- <span
            class="attr">发布于：<span>2019-09-06 14:33:32</span></span> -->
        <span class="attr">发布于：<span>2019-09-06</span></span>
        <span class="attr"><a class="tag" href="/blog/author/#陈则衔"
                title="陈则衔">陈则衔</a></span>
        
        <span class="attr">/
            
            <a class="tag" href="/blog/tags/#报告" title="报告">报告</a>
            <span>/</span>
            
            <a class="tag" href="/blog/tags/#论文评述" title="论文评述">论文评述</a>
            <span>/</span>
            
            
        </span>
        <span class="attr">访问：<span id="busuanzi_value_page_pv"></span>
        </span>
        </span>
    </div>
    <div class="post-content ">
        <h3 id="Markov-Process"><a href="#Markov-Process" class="headerlink" title="Markov Process"></a>Markov Process</h3><h4 id="Definition"><a href="#Definition" class="headerlink" title="Definition"></a>Definition</h4><ul>
<li>元组 $(\mathcal{S,P})$</li>
<li>$\mathcal{S}$是一个有限状态的集合</li>
<li>$\mathcal{P}$是一个状态转移矩阵：$\mathcal{P_{ss’}}=\mathbb{P}[\mathcal{S_t+1}=s’|\mathcal{S_t}=s]$</li>
</ul>
<h3 id="Markov-Reward-Process"><a href="#Markov-Reward-Process" class="headerlink" title="Markov Reward Process"></a>Markov Reward Process</h3><ul>
<li>在前者的基础上，增加了$(\mathcal{S,P,\color{red}{\mathcal{R,\gamma}}})$</li>
<li>$\color{red}{\mathcal{R}是一个奖励函数，\mathcal{R_s}=\mathbb{E}[R_{t+1}|\mathcal{S}=s]}$</li>
<li>$\color{red}\gamma是一个衰减因子，\gamma\in[0,1]$</li>
</ul>
<p>奖励函数$\mathcal{R}$代表了从状态$s$转移到状态$s’$时获得的奖励，这里奖励是离开状态后得到的(至于离开得到奖励还是进入一个新状态得到奖励只是定义了一种获得的规则而已)</p>
<h4 id="G-t"><a href="#G-t" class="headerlink" title="$G_t$"></a>$G_t$</h4><script type="math/tex; mode=display">
G_t=R_{t+1}+\gamma R_{t+2}+...=\mathcal{\sum_{k=0}^{\gamma^k}}R_{t+k+1}</script><p>代表了从状态$s$到最后状态$s_t$，得到的最终奖励，加入$\gamma$是因为距离越远，影响越小。即某一个具体episode所获得的return。</p>
<p>目标是将其最大化。</p>
<h4 id="价值函数-Value-Function"><a href="#价值函数-Value-Function" class="headerlink" title="价值函数(Value Function)"></a>价值函数(Value Function)</h4><script type="math/tex; mode=display">
v(s)=\Bbb{E}[G_t|\mathcal{S_t}=s]</script><p>其代表着在状态$s$下的，$G_t$的期望值，因为从一个状态s出发有很多种不同的决策路径，得到不同的$G_t$</p>
<h4 id="贝尔曼方程-Bellman-equation"><a href="#贝尔曼方程-Bellman-equation" class="headerlink" title="贝尔曼方程(Bellman equation)"></a>贝尔曼方程(<a href="https://en.wikipedia.org/wiki/Bellman_equation" target="_blank" rel="noopener">Bellman equation</a>)</h4><p><img src="http://www.cad.zju.edu.cn/home/vagblog/images/photo_bed/2019/9/23/26e48db0806441779d67fb70d06149ceb13d5f57.jpeg" alt></p>
<p>最后一行理由为：x的期望的期望是x期望其本身.得到了一个重要的<strong>递归</strong>公式:</p>
<script type="math/tex; mode=display">
\begin{align}
v(s) &= \Bbb{E}[R_{t+1}+\gamma v(\mathcal{S_{t+1}})|\mathcal{S_t}=s] \\
&=\mathcal{R_s}+\gamma \sum_{s'\in S}\mathcal{P}_{ss'}v(s')
\end{align}</script><p>其有两部分组成，及时奖励的期望$\boldsymbol{R_{t+1}}$  ,下一个时刻状态$\boldsymbol{s_{t+1}}$ 的期望</p>
<h4 id="矩阵求解"><a href="#矩阵求解" class="headerlink" title="矩阵求解"></a>矩阵求解</h4><p><img src="http://www.cad.zju.edu.cn/home/vagblog/images/photo_bed/2019/9/23/0e260df4c3dcb2edacb8aa623869cbbf38489bc5.png" alt></p>
<h3 id="Markov-Decision-Process"><a href="#Markov-Decision-Process" class="headerlink" title="Markov Decision Process"></a>Markov Decision Process</h3><p><img src="http://www.cad.zju.edu.cn/home/vagblog/images/photo_bed/2019/9/23/b63cb3ae94de7ee1b2def12376e148cc76b44eb9.png" alt></p>
<h3 id="策略-policy"><a href="#策略-policy" class="headerlink" title="策略(policy)"></a>策略(policy)</h3><p><img src="http://www.cad.zju.edu.cn/home/vagblog/images/photo_bed/2019/9/23/e60ff0160a4c1bf5dd56d7da59ff14744cdf2d4c.png" alt></p>
<p>策略代表了在给定状态$s$下，可能的动作概率分布。</p>
<p><img src="http://www.cad.zju.edu.cn/home/vagblog/images/photo_bed/2019/9/23/ea03b301ad52dabdb1b6a3d3d574f52a8ade00bf.png" alt></p>
<h3 id="价值函数-2"><a href="#价值函数-2" class="headerlink" title="价值函数-2"></a>价值函数-2</h3><p><img src="http://www.cad.zju.edu.cn/home/vagblog/images/photo_bed/2019/9/23/cebfbeac0473b101e8e2b6961a3956c5561cfcb2.png" alt></p>
<script type="math/tex; mode=display">
\begin{align}
v_\pi(s)&=\sum_{a\in\mathcal{A}}\pi(a|s)q_\pi(s,a) \\
q_\pi(s,a)&=\mathcal{R}_s^a+\gamma\sum_{s'\in\mathcal{S}}\mathcal{P}_{ss'}^av_\pi(s')
\end{align}</script><p>===&gt;</p>
<script type="math/tex; mode=display">
\begin{align}
v_\pi(s)&=\sum_{a\in\mathcal{A}}\pi(a|s)(\mathcal{R}_s^a+\gamma\sum_{s'\in\mathcal{S}}\mathcal{P}_{ss'}^av_\pi(s'))\\
q_\pi(s,a)&=\mathcal{R}_s^a+\gamma\sum_{s'\in\mathcal{S}}\mathcal{P}_{ss'}^a\sum_{a\in\mathcal{A}}\pi(a'|s')q_\pi(s',a')
\end{align}</script><p>可以发现，也是个递归地过程</p>
<p>$v_\pi(s)$是由当前状态$s$下，策略$\pi$可能的动作概率*该动作下得到的奖励值，累加而成</p>
<p>$q_\pi(s,a)$由两部分组成，及时回报和执行这个操作后可能到达所有状态$s’$的价值函数的累加</p>
<h3 id="将其最优化"><a href="#将其最优化" class="headerlink" title="将其最优化"></a>将其最优化</h3><p><img src="http://www.cad.zju.edu.cn/home/vagblog/images/photo_bed/2019/9/23/fcfed2f0d035736b586dea4629cd4495a84cb73f.png" alt></p>
<h3 id="最优策略"><a href="#最优策略" class="headerlink" title="最优策略"></a>最优策略</h3><p><img src="http://www.cad.zju.edu.cn/home/vagblog/images/photo_bed/2019/9/23/271d2386628b35d246a88720d96f92399a5f2aa5.png" alt></p>
<h3 id="最优状态动作价值函数"><a href="#最优状态动作价值函数" class="headerlink" title="最优状态动作价值函数"></a>最优状态动作价值函数</h3><p><img src="http://www.cad.zju.edu.cn/home/vagblog/images/photo_bed/2019/9/23/9285e8b93a21b00c61499323dfa866000fa725f1.png" alt></p>
<p>彼此带入：</p>
<p><img src="http://www.cad.zju.edu.cn/home/vagblog/images/photo_bed/2019/9/23/0a36903192cf4db9b154eee65dbf817598ac8eb0.png" alt></p>
<h3 id="如何求解"><a href="#如何求解" class="headerlink" title="如何求解"></a>如何求解</h3><p>得到最优解的递归形式，如何求解就很关键了。主要方法有：value Function(Q-learning,Sarsa);Policy gradient(PPO);AC等等.</p>
<p><img src="http://www.cad.zju.edu.cn/home/vagblog/images/photo_bed/2019/9/23/fd34e0959d91c416d55eba2a406924d9f09f1a1d.png" alt></p>
<p><em>Fig.  Summary of approaches in RL based on whether we want to model the value, policy, or the environment. (Image source: reproduced from David Silver’s RL course lecture 1.)</em> </p>
<h3 id="On-policy-vs-Off-policy"><a href="#On-policy-vs-Off-policy" class="headerlink" title="On-policy vs Off-policy"></a>On-policy vs Off-policy</h3><ul>
<li><strong>Model-based</strong>: Rely on the model of the environment; either the model is known or the algorithm learns it explicitly.(<em>When we fully know the environment, we can find the optimal solution by <a href="https://en.wikipedia.org/wiki/Dynamic_programming" target="_blank" rel="noopener">Dynamic Programming</a> (DP).</em>)</li>
<li><strong>Model-free</strong>: No dependency on the model during learning.</li>
<li><strong>Model-based</strong>尝试着model整个环境；先model了这个环境，基于该环境做出最优的策略；<strong>Model-free</strong>就是走一步看一步，在每一步中去尝试学习最优的策略。</li>
<li><em>The model-based learning uses environment, action and reward to get the most reward from the action. The model-free learning only uses its action and reward to infer the best action.</em></li>
</ul>
<ul>
<li><strong>On-policy</strong>: The agent learned and the agent interacting with the environment is the same.(<strong>自己和环境互动</strong>)</li>
<li><strong>Off-policy</strong>:The agent learned and the agent interacting with the environment is different.(<strong>自己看别人玩</strong>)</li>
</ul>
<hr>
<h3 id="Policy"><a href="#Policy" class="headerlink" title="Policy"></a>Policy</h3><p>Policy $\pi$,代表在状态$s$，会执行的动作$a$.(分为确定性和随机性)</p>
<ul>
<li>Deterministic: $\pi(s)=a.$</li>
<li>Stochastic: $\pi(a|s)=\mathbb{P}_\pi[A=a|S=s]$</li>
<li>$\pi_\theta(a|s)=\mathbb{P}_\pi[A=a|S=s,\theta]$</li>
</ul>
<h4 id="Policy-Gradient"><a href="#Policy-Gradient" class="headerlink" title="Policy Gradient"></a>Policy Gradient</h4><p><strong>object function</strong>:</p>
<p><strong>episodic environments:</strong></p>
<p>$J_1(\theta)=V^{\pi_\theta}(s_1)=\mathbb{E}_{\pi_\theta}[V_1]$</p>
<p><strong>continuing environments:</strong></p>
<p><em>连续型环境就没有初始状态一说了，那么就取平均，考虑agent在某时刻处于某个状态下的概率，即状态分布</em></p>
<p><strong>average value:</strong></p>
<p>$J_{avV}(\theta)=\sum_sd^{\pi_\theta}(s)V^{\pi_\theta}(s)$</p>
<p>对每个可能的状态计算从该时刻(该状态)开始于环境一直交互下去的奖励，然后对其概率分布求和.</p>
<p><strong>average reward per time-step:</strong></p>
<p>$J_{avR}(\theta)=\sum_sd^{\pi_\theta}(s)\sum_a\pi_\theta(s,a)\mathcal{R}_s^a$</p>
<p>其中,$d^{\pi_\theta}(s)$是马尔科夫链在$\pi_\theta$下的随机概率分布；即$d^\pi(s)=lim_{t→∞}P(s_t=s|s_0,\pi_θ)$就表示你从状态$s_0$开始，在策略$\pi_θ$下经过$t$个时间步后到达状态$s$的概率。</p>
<p>该式代表了到达某个状态$s$,并且采用动作$a$的情况下，获得的平均回报$\mathcal{R_s^a}$,概率化累加就是平均回报.</p>
<p>这个可以如下计算(以李宏毅教授PPT为例)，每条迹的概率*该条迹累计的期望即为平均</p>
<p><img src="http://www.cad.zju.edu.cn/home/vagblog/images/photo_bed/2019/9/23/2d99f29215e2162298d27f45456142a4ed46ab00.png" alt></p>
<p>目标是将目标函数最大化，可以用梯度下降法将其最大化</p>
<p><em>注：用基于梯度的策略优化时，是基于序列结构片段，如果无穷地和环境交互，得到一个结果是没有意义的；就是和环境交互中的某一个序列，将其拿出来进行学习，然后优化策略</em></p>
<p><strong>one-step MDP(per time-step):</strong></p>
<script type="math/tex; mode=display">
\begin{split}
J(\theta)&=\sum_sd^{\pi_\theta}(s)\sum_a\pi_\theta(s,a)\mathcal{R}_s^a  
\\
\nabla_\theta J(\theta)&=\sum_sd^{\pi_\theta}(s)\sum_a\pi_\theta(s,a)\nabla_\theta log\pi_\theta(s,a)\mathcal{R}_s^a
\end{split}</script><h4 id="gt-policy-gradient-theorem"><a href="#gt-policy-gradient-theorem" class="headerlink" title="==&gt;policy gradient theorem"></a>==&gt;policy gradient theorem</h4><p><img src="http://www.cad.zju.edu.cn/home/vagblog/images/photo_bed/2019/9/23/3741b1b76e122866288c4b7ee69add9749fd1988.png" alt></p>
<h4 id="gt-李宏毅教授PPT"><a href="#gt-李宏毅教授PPT" class="headerlink" title="==&gt;李宏毅教授PPT"></a>==&gt;李宏毅教授PPT</h4><p><img src="http://www.cad.zju.edu.cn/home/vagblog/images/photo_bed/2019/9/23/74a31be33bfec962a642414a023273ca7f0567c1.png" alt="AC-3"></p>
<p><br></p>
<h3 id="Actor-Critic"><a href="#Actor-Critic" class="headerlink" title="Actor-Critic"></a>Actor-Critic</h3><p>AC是在基于Policy Gradient的基础上进行了变化，融合了Value based的方法.</p>
<p>Actor:就是把$J(\theta)$最大化，但是梯度里面的$Q^{\pi_\theta}(s,a)$用Value based的方法来计算，结果如下：</p>
<p><img src="http://www.cad.zju.edu.cn/home/vagblog/images/photo_bed/2019/9/23/85df5905cf5e757bfb93f10f49b0b7908d9a440b.png" alt="AC-4"></p>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p><a href="https://blog.csdn.net/liweibin1994/article/details/79079884" target="_blank" rel="noopener">csdn-blog</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/28084942" target="_blank" rel="noopener">David Silver强化学习公开课</a></p>
<p>David Silver slides</p>
<p>博客:</p>
<p><a href="https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html#deadly-triad-issue" target="_blank" rel="noopener">https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html#deadly-triad-issue</a></p>
<p><a href="https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html" target="_blank" rel="noopener">https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html</a></p>
<p>李宏毅教授：</p>
<p>David_Silver:</p>
<p>配套PPT</p>
<p>Mnih, Volodymyr, et al. <a href="https://arxiv.org/abs/1602.01783" target="_blank" rel="noopener">“Asynchronous methods for deep reinforcement learning.”</a> ICML. 2016.</p>
<p>David Silver, et al. <a href="https://hal.inria.fr/file/index/docid/938992/filename/dpg-icml2014.pdf" target="_blank" rel="noopener">“Deterministic policy gradient algorithms.”</a> ICML. 2014.</p>
<p>Timothy P. Lillicrap, et al. <a href="https://arxiv.org/pdf/1509.02971.pdf" target="_blank" rel="noopener">“Continuous control with deep reinforcement learning.”</a> arXiv preprint arXiv:1509.02971 (2015).</p>

        
        <br />
        <div id="comment-container">
        </div>
        <div id="disqus_thread"></div>

        <div id="lv-container">
        </div>

    </div>
</div>
    </div>
</div>


<footer class="footer">
    <ul class="list-inline text-center">
        
        

        

        

        

        

    </ul>
    
    <p>
        <span id="busuanzi_container_site_pv">
            <span id="busuanzi_value_site_pv"></span>PV
        </span>
        <span id="busuanzi_container_site_uv">
            <span id="busuanzi_value_site_uv"></span>UV
        </span>
        Created By <a href="https://hexo.io/">Hexo</a>  Theme <a href="https://github.com/aircloud/hexo-theme-aircloud">AirCloud</a></p>
</footer><!-- hexo-inject:begin --><!-- hexo-inject:end -->




</body>

<script>
    // We expose some of the variables needed by the front end
    window.hexo_search_path = "search.json"
    window.hexo_root = "/blog/"
    window.isPost = true
</script>
<script src="https://cdn.bootcss.com/jquery/3.3.1/jquery.min.js"></script>
<script src="/blog/js/index.js"></script>
<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




    <script type="text/javascript">
       (function() {
           if (typeof LivereTower === 'function') { return; }

           var j, d = document.getElementById('lv-container');

           d.setAttribute('data-id','city');
           d.setAttribute('data-uid' , 'MTAyMC8yOTk2MC82NTI1');

           j = document.createElement('script');
           j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
           j.async = true;

           d.appendChild(j);
       })();
    </script>
    <noscript>为正常使用来必力评论功能请激活JavaScript</noscript>
    </div>

</html>
